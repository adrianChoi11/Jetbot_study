{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영상을 받아들일 카메라 인스턴스 생성 \n",
    "\n",
    "from jetbot import Camera \n",
    "from jetbot import bgr8_to_jpeg\n",
    "\n",
    "camera = Camera.instance(width=720, height=720)  # 720x720 픽셀 이미지 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "serial Open!\n",
      "83\n",
      "b'\\xff\\xff\\xfe\\x0e\\x83*\\x04\\x01\\x08\\x00\\x00\\n\\x02\\x08\\xc8\\x00\\nS'\n"
     ]
    }
   ],
   "source": [
    "# 서보모터의 초기화 위치 설정\n",
    "# sudo chmod 666 /dev/ttyTHS1 해당 명령어로 원한 수정 해주어야함\n",
    "\n",
    "\n",
    "from servoserial import ServoSerial\n",
    "import time\n",
    "\n",
    "pantilt = ServoSerial()  # Pantilt 서보 인스턴스 생성\n",
    "\n",
    "# 초기 카메라 방향을 좌우의 중간, 상하의 약간 위를 바라보도록 설정\n",
    "pantilt.Servo_serial_double_control(1,2048,2,2048+200)\n",
    "time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검출 영역의 좌표와 크기\n",
    "# 검출 대상인 얼굴이 검출 된 좌표의 크기와 영역, 나중에 박스로 표시함\n",
    "global detect_x, detect_y, detect_w, detect_h\n",
    "\n",
    "detect_x = detect_y = detect_w = detect_h = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c44f6d7a07546a2af797c8a017b8c2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image(value=b'', format='jpeg', height='300', width='300')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 키메라 프리뷰 위젯 만들기\n",
    "\n",
    "import traitlets\n",
    "import ipywidgets.widgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "detect_img = widgets.Image(format='jpeg', width=300, height=300)  # 프리뷰 화면은 300x300 픽셀로 설정\n",
    "display(detect_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HARR Ccascade Classifier 학습 데이터 로딩\n",
    "#아래 셋 중 하나 골라서 사용\n",
    "\n",
    "import cv2\n",
    "\n",
    "face_detect_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')  # 정면 얼굴 검출용, 해당 파일을 같은 다렉토리에 둔 후 작업한다, 아니면 쳥로를 찾아서 정확하게 입력한다\n",
    "eye_detect_cascade = cv2.CascadeClassifier('haarcascade_eye_tree_eyeglasses.xml')  # 눈 검출용\n",
    "# https://www.kaggle.com/lalitharajesh/haarcascades/version/1\n",
    "# detect_cascade = cv2.CascadeClassifier('haarcascade_fullbody.xml')  # 몸 전체 검출용\n",
    "# detect_cascade = cv2.CascadeClassifier('haarcascade_upperbody.xml')  # 상반신 검출용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-63b1f6c89def>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mgray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 흑백 이미지로 변환하여 검출에 사용\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mfaces\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mface_detect_cascade\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetectMultiScale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgray\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 이미지 내에서 얼굴을 검출\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfaces\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# 검출된 내용이 있으면\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    frame = camera.value\n",
    "    frame= cv2.resize(frame,(300,300))  # 이미지를 학습 데이터와 동일한 크기로 변환\n",
    "    gray = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)  # 흑백 이미지로 변환하여 검출에 사용\n",
    "    \n",
    "    faces = face_detect_cascade.detectMultiScale(gray)  # 이미지 내에서 얼굴을 검출\n",
    "    \n",
    "    if len(faces) > 0:  # 검출된 내용이 있으면        \n",
    "        (face_x, face_y, face_w, face_h) = faces[0]  # 좌표와 크기\n",
    "        \n",
    "        # 검출 영역을 사각 박스로 표시합니다.\n",
    "        cv2.rectangle(frame, (face_x, face_y),(face_x+face_w, face_y+face_h), (0,255,123), 5)\n",
    "        \n",
    "        # 눈 검출은 얼굴이 검출된 영역 내부에서만 진행하기 위해 ROI(이미지 내 관심영역, Region of Interest)를 설정합니다\n",
    "        roi_gray = gray[face_y:face_y+face_h, face_x:face_x+face_w]\n",
    "        roi_color = frame[face_y:face_y+face_h, face_x:face_x+face_w]\n",
    "        \n",
    "        # 얼굴 안에서 눈을 검출합니다.\n",
    "        eyes = eye_detect_cascade.detectMultiScale(roi_gray)  # 4개의 백터가 나옴\n",
    "        \n",
    "        # 눈이 검출 되었다면 눈의 위치에 대한 좌표 정보를 리턴합니다.\n",
    "        for (ex,ey,ew,eh) in eyes:\n",
    "            \n",
    "            # 원본 이미지에 눈의 위치를 표시합니다.\n",
    "            cv2.rectangle(roi_color, (ex,ey), (ex+ew, ey+eh),(255,0,123), 2)\n",
    "            \n",
    "    # 결과 이미지를 프리뷰 위젯에 표시\n",
    "    detect_img.value = bgr8_to_jpeg(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[144 140 143 ... 185 183 185]\n",
      " [146 145 147 ... 183 183 185]\n",
      " [145 146 145 ... 185 184 183]\n",
      " ...\n",
      " [ 39  38  39 ... 121 124 127]\n",
      " [ 38  34  38 ... 119 122 122]\n",
      " [ 37  33  35 ... 119 123 123]]\n",
      "[[[165 143 139]\n",
      "  [166 140 130]\n",
      "  [172 143 132]\n",
      "  ...\n",
      "  [217 183 177]\n",
      "  [215 176 183]\n",
      "  [210 184 178]]\n",
      "\n",
      " [[173 146 136]\n",
      "  [170 146 134]\n",
      "  [172 148 137]\n",
      "  ...\n",
      "  [215 180 176]\n",
      "  [214 181 175]\n",
      "  [215 182 178]]\n",
      "\n",
      " [[170 144 138]\n",
      "  [173 146 137]\n",
      "  [171 147 132]\n",
      "  ...\n",
      "  [216 183 176]\n",
      "  [216 182 175]\n",
      "  [216 181 173]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 51  36  40]\n",
      "  [ 50  37  37]\n",
      "  [ 53  37  39]\n",
      "  ...\n",
      "  [138 121 114]\n",
      "  [140 124 117]\n",
      "  [149 125 122]]\n",
      "\n",
      " [[ 49  37  35]\n",
      "  [ 53  33  29]\n",
      "  [ 49  33  43]\n",
      "  ...\n",
      "  [139 115 119]\n",
      "  [141 120 119]\n",
      "  [148 120 117]]\n",
      "\n",
      " [[ 49  35  37]\n",
      "  [ 39  34  29]\n",
      "  [ 48  33  34]\n",
      "  ...\n",
      "  [137 117 116]\n",
      "  [143 121 119]\n",
      "  [148 119 122]]]\n"
     ]
    }
   ],
   "source": [
    "print(gray)\n",
    "print(frame)   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
